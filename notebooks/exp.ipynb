{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('d:\\\\vscode_machineLearning\\\\internship\\\\sentiment-Analysis-fellowship.ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df = pd.read_csv('sentiment-analysis-dataset/IMDB_clean_data.csv',index_col=False)\n",
    "df = df.drop('Unnamed: 0',axis=1) # droping the Unnamed: 0\n",
    "df.dropna(inplace=True) # I had one NaN value in my dataframe.\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build corpus:   0%|          | 9/49999 [00:00<01:29, 558.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['one review mention watch oz episod exactli happen first thing struck oz brutal unflinch scene set right word trust show faint heart show pull punch regard sex classic use call oz nicknam given oswald maximum secur state focus mainli emerald experiment section prison cell glass front face privaci high em citi home irish death dodgi deal shadi agreement never far would say main appeal show due fact goe show forget pretti pictur paint mainstream forget forget mess first episod ever saw struck nasti say readi watch develop tast got accustom high level graphic injustic guard sold inmat kill order get away well middl class inmat turn prison bitch due lack street skill prison watch may becom comfort uncomfort get touch darker',\n",
       " 'wonder littl film techniqu fashion give sometim sens realism entir actor extrem well michael sheen got voic pat truli see seamless edit guid refer diari well worth watch terrificli written perform master product one great comedi realism realli come home littl fantasi guard rather use tradit techniqu remain solid play knowledg particularli scene concern orton halliwel set flat mural decor everi terribl well',\n",
       " 'thought wonder way spend time hot summer sit air condit theater watch plot dialogu witti charact likabl well bread suspect serial may disappoint realiz match point risk thought proof woodi allen still fulli control style mani us grown laugh one comedi year say never impress scarlet manag tone imag jump right spirit young may crown jewel wittier wear interest great comedi go see',\n",
       " 'basic famili littl boy think zombi closet parent fight movi slower soap jake decid becom rambo kill first go make film must decid thriller drama movi parent divorc argu like real jake closet total ruin expect see boogeyman similar instead watch drama meaningless thriller well play parent descent shot ignor',\n",
       " 'petter time visual stun film mattei offer us vivid portrait human movi seem tell us power success peopl differ situat variat arthur play director transfer action present time new york differ charact meet one connect one anoth next one seem know previou point film sophist luxuri taken see peopl live world live thing one get soul pictur differ stage loneli one big citi exactli best place human relat find sincer one discern case peopl act good steve rosario carol michael adrian rest talent make charact come wish mattei good luck await anxious next',\n",
       " 'probabl favorit stori sacrific dedic nobl preachi never get despit seen time last paul perform bring tear bett one truli sympathet kid grandma like make fun slow awaken happen world roof believ dozen',\n",
       " 'sure would like see resurrect date seahunt seri tech today would bring back kid excit grew black white tv seahunt gunsmok everi vote comeback new sea need chang pace tv would work world water way thank outlet like view mani viewpoint tv mani ole way believ got wanna nice read plu point sea rhyme would line would let leav doubt must go let',\n",
       " 'show fresh innov idea first first year thing drop show realli funni continu declin complet wast time truli disgrac far show write pain perform almost bad mildli entertain respit show probabl still find hard believ creator origin cast also chose band hack one recogn brillianc see fit replac felt must give star respect origin cast made show huge show believ still',\n",
       " 'encourag posit comment film look forward watch bad seen film truli one worst aw almost everi soundtrack song lame countri tune play less four film look cheap nasti bore rare happi see end credit thing prevent give harvey keitel far best perform least seem make bit one keitel obsess',\n",
       " 'like origin gut wrench laughter like young old love hell even mom like']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "count = 0\n",
    "for i in tqdm(range(0,len(X)),desc =\"build corpus\"):\n",
    "    stemmed_words = [ps.stem(word) for word in X[i].split()]\n",
    "    text = ' '.join(stemmed_words)\n",
    "    corpus.append(text)\n",
    "    count +=1\n",
    "    if count == 10:\n",
    "        break\n",
    "    \n",
    "    # print(' '.join(stemmed_words))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x20113775990>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_docs = tokenizer.texts_to_sequences(texts=corpus) # I am assigning number to text. Transforming each text in texts to a sequence of integers.\n",
    "len(corpus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 99,\n",
       " 100,\n",
       " 3,\n",
       " 18,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 9,\n",
       " 11,\n",
       " 49,\n",
       " 18,\n",
       " 101,\n",
       " 102,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 103,\n",
       " 104,\n",
       " 2,\n",
       " 105,\n",
       " 106,\n",
       " 2,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 53,\n",
       " 112,\n",
       " 18,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 19,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 54,\n",
       " 129,\n",
       " 55,\n",
       " 56,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 20,\n",
       " 21,\n",
       " 7,\n",
       " 22,\n",
       " 136,\n",
       " 137,\n",
       " 2,\n",
       " 57,\n",
       " 138,\n",
       " 139,\n",
       " 2,\n",
       " 23,\n",
       " 140,\n",
       " 58,\n",
       " 141,\n",
       " 142,\n",
       " 23,\n",
       " 23,\n",
       " 143,\n",
       " 9,\n",
       " 46,\n",
       " 144,\n",
       " 145,\n",
       " 49,\n",
       " 59,\n",
       " 22,\n",
       " 146,\n",
       " 3,\n",
       " 147,\n",
       " 148,\n",
       " 24,\n",
       " 149,\n",
       " 54,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 60,\n",
       " 153,\n",
       " 61,\n",
       " 62,\n",
       " 154,\n",
       " 12,\n",
       " 155,\n",
       " 8,\n",
       " 156,\n",
       " 157,\n",
       " 61,\n",
       " 158,\n",
       " 19,\n",
       " 159,\n",
       " 57,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 19,\n",
       " 3,\n",
       " 25,\n",
       " 63,\n",
       " 163,\n",
       " 164,\n",
       " 12,\n",
       " 165,\n",
       " 166]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my vocab has 416 unique words\n"
     ]
    }
   ],
   "source": [
    "print(f' my vocab has {len(tokenizer.word_index)} unique words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 250)\n"
     ]
    }
   ],
   "source": [
    "# adding zero's end of the sequences.\n",
    "padded_docs = pad_sequences(sequences=corpus_docs,maxlen=250,padding='post')\n",
    "pprint(padded_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(padded_docs).to_csv(r'sentiment-analysis-dataset/padded_docs.csv')\n",
    "# type(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using in a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewers mentioned watching oz episode exactly happened first thing struck oz brutality unflinching scenes set right word trust show faint hearted show pulls punches regards sex classic use called oz nickname given oswald maximum security state focuses mainly emerald experimental section prison cells glass fronts face privacy high em city home irish death dodgy dealings shady agreements never far would say main appeal show due fact goes shows forget pretty pictures painted mainstream forget forget mess first episode ever saw struck nasty say ready watched developed taste got accustomed high levels graphic injustice guards sold inmates kill order get away well middle class inmates turned prison bitches due lack street skills prison watching may become comfortable uncomfortable get touch darker'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = X[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one reviewers mentioned watching oz episode exactly happened first thing '\n",
      " 'struck oz brutality unflinching scenes set right word trust show faint '\n",
      " 'hearted show pulls punches regards sex classic use called oz nickname given '\n",
      " 'oswald maximum security state focuses mainly emerald experimental section '\n",
      " 'prison cells glass fronts face privacy high em city home irish death dodgy '\n",
      " 'dealings shady agreements never far would say main appeal show due fact goes '\n",
      " 'shows forget pretty pictures painted mainstream forget forget mess first '\n",
      " 'episode ever saw struck nasty say ready watched developed taste got '\n",
      " 'accustomed high levels graphic injustice guards sold inmates kill order get '\n",
      " 'away well middle class inmates turned prison bitches due lack street skills '\n",
      " 'prison watching may become comfortable uncomfortable get touch dark')\n"
     ]
    }
   ],
   "source": [
    "## stem\n",
    "stemmed_txt = ps.stem(text)\n",
    "pprint(stemmed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(stemmed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts=stemmed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "docs = tokenizer.fit_on_sequences(stemmed_txt)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.chdir('d:\\\\vscode_machineLearning\\\\internship\\\\sentiment-Analysis-fellowship.ai')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-analysis-dataset/IMDB_clean_data.csv',index_col=False)\n",
    "df = df.drop('Unnamed: 0',axis=1) # droping the Unnamed: 0\n",
    "df.dropna(inplace=True) # I had one NaN value in my dataframe.\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pd.read_csv('sentiment-analysis-dataset/padded_docs.csv')\n",
    "padded_docs = padded_docs.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = padded_docs.iloc[:,:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = np.array(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df ['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     np.array(padded_docs), np.array(y), test_size=0.23, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (38499, 150)\n",
      "X_test shape: (11500, 150)\n",
      "y_train shape: (38499,)\n",
      "y_test shape: (11500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Dropout, Bidirectional, LSTM, Embedding, Dense \n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import LearningRateScheduler , EarlyStopping\n",
    "from keras.activations import relu , sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = 100 # for embedding layer\n",
    "input_len = 150\n",
    "model = keras.Sequential(name='LSTM_model')\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=56942,  # The \"+1\" accounts for the reserved index 0 in the word index (since word indices start from 1 and not 0)\n",
    "    output_dim=model_features, input_length=input_len,name = 'input_layer'\n",
    "))\n",
    "model.add(Bidirectional(\n",
    "    LSTM(units=64,activation=relu,return_sequences=True),\n",
    "    name='LSTM_1'\n",
    "))\n",
    "model.add(\n",
    "    Dropout(rate=0.5))\n",
    "model.add(\n",
    "    BatchNormalization())\n",
    "model.add(\n",
    "    Dropout(rate=0.5))\n",
    "model.add(Bidirectional(\n",
    "    LSTM(units=32,activation=relu,return_sequences=False),\n",
    "    name='LSTM_2'\n",
    "))\n",
    "model.add(Dense(\n",
    "    units=128,activation=relu,name='fully_connected_layer'\n",
    "))\n",
    "model.add(Dense(\n",
    "    units=1,activation=sigmoid,name='output_layer'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (Embedding)     (None, 150, 100)          5694200   \n",
      "                                                                 \n",
      " LSTM_1 (Bidirectional)      (None, 150, 128)          84480     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 150, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " LSTM_2 (Bidirectional)      (None, 64)                41216     \n",
      "                                                                 \n",
      " fully_connected_layer (Dens  (None, 128)              8320      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,828,857\n",
      "Trainable params: 5,828,601\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 1:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "# learning rate scheduler callback to descrese the learning rate gradually as the epochs increases So that my alogrithm could not jump out of Global minima.\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Early stopping to stop the Neural Network when we get same Validation accuracy\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    min_delta=0.00001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, # Used Adam because this has not any major disadvantages with custom learning rate because the convergence was very unstable.\n",
    "               loss=binary_crossentropy, # because solving the classification problem\n",
    "                 metrics=['accuracy'])  # I don't need to write about this you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1204/1204 [==============================] - 496s 403ms/step - loss: 40354640.0000 - accuracy: 0.6660 - val_loss: 0.5525 - val_accuracy: 0.7262 - lr: 5.0000e-04\n",
      "Epoch 2/20\n",
      "1204/1204 [==============================] - 445s 370ms/step - loss: 56.9634 - accuracy: 0.7304 - val_loss: 0.5775 - val_accuracy: 0.7030 - lr: 4.5242e-04\n",
      "Epoch 3/20\n",
      "1204/1204 [==============================] - 444s 368ms/step - loss: 0.5106 - accuracy: 0.7487 - val_loss: 0.5684 - val_accuracy: 0.7084 - lr: 4.0937e-04\n",
      "Epoch 4/20\n",
      "1204/1204 [==============================] - 487s 404ms/step - loss: 0.4793 - accuracy: 0.7649 - val_loss: 0.5720 - val_accuracy: 0.7023 - lr: 3.7041e-04\n",
      "Epoch 5/20\n",
      "1204/1204 [==============================] - 481s 400ms/step - loss: 0.4575 - accuracy: 0.7799 - val_loss: 0.5753 - val_accuracy: 0.7010 - lr: 3.3516e-04\n",
      "Epoch 6/20\n",
      "1204/1204 [==============================] - 487s 405ms/step - loss: 0.6648 - accuracy: 0.7898 - val_loss: 0.5773 - val_accuracy: 0.7034 - lr: 3.0327e-04\n",
      "Epoch 7/20\n",
      "1204/1204 [==============================] - 512s 426ms/step - loss: 0.4392 - accuracy: 0.7946 - val_loss: 0.5795 - val_accuracy: 0.7064 - lr: 2.7441e-04\n",
      "Epoch 8/20\n",
      "1204/1204 [==============================] - 441s 366ms/step - loss: 0.4178 - accuracy: 0.8011 - val_loss: 0.5829 - val_accuracy: 0.7088 - lr: 2.4829e-04\n",
      "Epoch 9/20\n",
      "1204/1204 [==============================] - 436s 363ms/step - loss: 0.4072 - accuracy: 0.8074 - val_loss: 0.5895 - val_accuracy: 0.7082 - lr: 2.2466e-04\n",
      "Epoch 10/20\n",
      "1204/1204 [==============================] - 425s 353ms/step - loss: 0.3855 - accuracy: 0.8194 - val_loss: 0.6170 - val_accuracy: 0.6900 - lr: 2.0328e-04\n",
      "Epoch 11/20\n",
      "1204/1204 [==============================] - 447s 371ms/step - loss: 2.5097 - accuracy: 0.8382 - val_loss: 0.5962 - val_accuracy: 0.7198 - lr: 1.8394e-04\n",
      "Epoch 12/20\n",
      "1204/1204 [==============================] - 441s 366ms/step - loss: 0.5567 - accuracy: 0.8435 - val_loss: 0.6051 - val_accuracy: 0.7169 - lr: 1.6644e-04\n",
      "Epoch 13/20\n",
      "1204/1204 [==============================] - 441s 366ms/step - loss: 0.3292 - accuracy: 0.8489 - val_loss: 0.6075 - val_accuracy: 0.7186 - lr: 1.5060e-04\n",
      "Epoch 14/20\n",
      "1204/1204 [==============================] - 392s 326ms/step - loss: 146.9574 - accuracy: 0.8459 - val_loss: 0.5993 - val_accuracy: 0.7250 - lr: 1.3627e-04\n",
      "Epoch 15/20\n",
      "1204/1204 [==============================] - 446s 371ms/step - loss: 0.7705 - accuracy: 0.8505 - val_loss: 0.6054 - val_accuracy: 0.7240 - lr: 1.2330e-04\n",
      "Epoch 16/20\n",
      "1204/1204 [==============================] - 413s 343ms/step - loss: 0.3432 - accuracy: 0.8562 - val_loss: 0.6107 - val_accuracy: 0.7268 - lr: 1.1157e-04\n",
      "Epoch 17/20\n",
      "1204/1204 [==============================] - 408s 339ms/step - loss: 0.3232 - accuracy: 0.8561 - val_loss: 0.6126 - val_accuracy: 0.7270 - lr: 1.0095e-04\n",
      "Epoch 18/20\n",
      "1204/1204 [==============================] - 356s 296ms/step - loss: 0.4780 - accuracy: 0.8590 - val_loss: 0.6131 - val_accuracy: 0.7266 - lr: 9.1342e-05\n",
      "Epoch 19/20\n",
      "1204/1204 [==============================] - 443s 368ms/step - loss: 0.3330 - accuracy: 0.8624 - val_loss: 0.6177 - val_accuracy: 0.7284 - lr: 8.2649e-05\n",
      "Epoch 20/20\n",
      "1204/1204 [==============================] - 385s 320ms/step - loss: 0.4015 - accuracy: 0.8655 - val_loss: 0.6196 - val_accuracy: 0.7292 - lr: 7.4784e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20,\n",
    "                    batch_size=32, # I had tried different batch sizes but this has given my best results\n",
    "                      callbacks=[lr_scheduler, early_stopping]) # these to prevent the NN from overfitting and scheduling learning rate to get optimum solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentimentAnalysisModel.H5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentimentAnalysisModel.H5\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sentimentAnalysisModel.H5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.chdir('d:\\\\vscode_machineLearning\\\\internship\\\\sentiment-Analysis-fellowship.ai')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-analysis-dataset/IMDB_clean_data.csv',index_col=False)\n",
    "df = df.drop('Unnamed: 0',axis=1) # droping the Unnamed: 0\n",
    "df.dropna(inplace=True) # I had one NaN value in my dataframe.\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pd.read_csv('sentiment-analysis-dataset/padded_docs.csv')\n",
    "padded_docs = padded_docs.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = np.array(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df ['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     np.array(padded_docs), np.array(y), test_size=0.23, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (38499, 250)\n",
      "X_test shape: (11500, 250)\n",
      "y_train shape: (38499,)\n",
      "y_test shape: (11500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Dropout, Bidirectional, LSTM, Embedding, Dense\n",
    "from keras.losses import binary_crossentropy,categorical_crossentropy\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import LearningRateScheduler , EarlyStopping\n",
    "from keras.activations import relu , softmax , sigmoid\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = 100 # for embedding layer\n",
    "input_len = 250\n",
    "model = keras.Sequential(name='LSTM_model')\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=56942,  # The \"+1\" accounts for the reserved index 0 in the word index (since word indices start from 1 and not 0)\n",
    "    output_dim=model_features, input_length=input_len,name = 'input_layer'\n",
    "))\n",
    "model.add(Bidirectional(\n",
    "    LSTM(units=64,activation=relu,return_sequences=True),\n",
    "    name='LSTM_1'\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(\n",
    "    LSTM(units=32,activation=relu,return_sequences=False),\n",
    "    name='LSTM_2'\n",
    "))\n",
    "\n",
    "model.add(Dense(\n",
    "    units=128,activation=relu,name='fully_connected_layer'\n",
    "))\n",
    "model.add(Dense(\n",
    "    units=1,activation=sigmoid,name='output_layer'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (Embedding)     (None, 250, 100)          5694200   \n",
      "                                                                 \n",
      " LSTM_1 (Bidirectional)      (None, 250, 128)          84480     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 250, 128)         512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " LSTM_2 (Bidirectional)      (None, 64)                41216     \n",
      "                                                                 \n",
      " fully_connected_layer (Dens  (None, 128)              8320      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,828,857\n",
      "Trainable params: 5,828,601\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 1:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "# learning rate scheduler callback to descrese the learning rate gradually as the epochs increases So that my alogrithm could not jump out of Global minima.\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Early stopping to stop the Neural Network when we get same Validation accuracy\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    min_delta=0.00001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, # Used Adam because this has not any major disadvantages with custom learning rate because the convergence was very unstable.\n",
    "               loss=binary_crossentropy, # because solving the classification problem\n",
    "                 metrics=['accuracy'])  # I don't need to write about this you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1204/1204 [==============================] - 635s 521ms/step - loss: 0.6547 - accuracy: 0.6843 - val_loss: 0.5531 - val_accuracy: 0.7124 - lr: 5.0000e-04\n",
      "Epoch 2/20\n",
      " 254/1204 [=====>........................] - ETA: 7:19 - loss: nan - accuracy: 0.6454"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20,\n",
    "                    batch_size=32, # I had tried different batch sizes but this has given my best results\n",
    "                      callbacks=[lr_scheduler, early_stopping]) # these to prevent the NN from overfitting and scheduling learning rate to get optimum solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('sentimentAnalysisModel.H5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

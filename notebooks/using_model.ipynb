{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\vscode_machineLearning\\internship\\sentiment-Analysis-fellowship.ai\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'd:\\\\vscode_machineLearning\\\\internship\\\\sentiment-Analysis-fellowship.ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(r'sentimentAnalysisModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (Embedding)     (None, 150, 100)          5694200   \n",
      "                                                                 \n",
      " LSTM_1 (Bidirectional)      (None, 150, 128)          84480     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 150, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " LSTM_2 (Bidirectional)      (None, 64)                41216     \n",
      "                                                                 \n",
      " fully_connected_layer (Den  (None, 128)               8320      \n",
      " se)                                                             \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5828857 (22.24 MB)\n",
      "Trainable params: 5828601 (22.23 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pd.read_csv(r'sentiment-analysis-dataset\\padded_docs.csv')\n",
    "padded_docs = padded_docs.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = padded_docs.iloc[:,:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs=np.array(padded_docs)\n",
    "padded_docs = np.array(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded_docs[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    3,   237,   317,     8,  3680,   216,   440,   131,    24,\n",
       "           32,  2294,  3680,   996,  9208,    19,    88,   134,   332,\n",
       "         1209,    21,  4362,   442,    21,   452,  1695,   937,   338,\n",
       "          263,    56,   130,  3680,  5646,   232, 10499,  4639,  1581,\n",
       "          534,   905,  1097, 13548,  2967,  1882,   802,  1939,  1845,\n",
       "          744,   251, 12671,   223,  6462,   517,   312,  1824,   300,\n",
       "         4908,   346,  5480,  7134,    38,   126,    10,    41,   157,\n",
       "          756,    21,   490,   122,   165,    21,   582,    80,   324,\n",
       "          972,  2025,   582,   582,   845,    24,   216,    51,    99,\n",
       "         2294,  1310,    41,  1207,     8,   373,  1049,    83,  6109,\n",
       "          223,   568,  1103,  4950,  1721,  2152,  4077,    91,   414,\n",
       "            6,   188,    44,   571,   676,  4077,    77,   802,  4419,\n",
       "          490,   224,   685,   998,   802,     8,    87,    97,  1609,\n",
       "         2375,     6,   423,  2947,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.998613]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs[[7865]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im big fan work many enjoyed movie postal im boll apparently bought rights use far cry long ago even game even people enjoyed killing mercs infiltrating secret research labs located tropical island far something mr boll schemed together along legion feeling loneley set mr boll invites three countrymen play players go names til udo kier ralf names actually made selfs pretty big movie tale goes like jack carver played til schweiger carver german hail bratwurst eating however find tils acting movie pretty people complained really staying true whole carver agenda saw carver first person perspective really know looked like kicking storyline film beyond see evil mad scientist krieger played udo making gms performing research island reminds vancouver thats right palm trees instead got nice rich even gone far started cry cannot go wanna stay true bolls shenanigans go see movie disappointed delivers true boll meaning things worth mentioning would imply boll good work areas film nice boat fighting whole gms squad enters scene everything makes movie far cry reeks scheisse poop wanna take wiff go btw carver gets annoying sidekick makes wanna shoot first three minutes\n"
     ]
    }
   ],
   "source": [
    "class preprocess_input:\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower()  # Initialize the text attribute\n",
    "    \n",
    "    # remove html tags\n",
    "    def remove_html_tags(self):\n",
    "        soup = BeautifulSoup(self.text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def remove_between_square_brackets(self):\n",
    "        return re.sub(r'http\\S+', '', self.text)\n",
    "    \n",
    "    def remove_stopwords(self):\n",
    "        stop = set(stopwords.words('english'))\n",
    "        final_text = []\n",
    "        for i in self.text.split():\n",
    "            if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "                final_text.append(i.strip().lower())\n",
    "        return \" \".join(final_text)\n",
    "    \n",
    "    def use_all(self):\n",
    "        txt = self.remove_html_tags()\n",
    "        txt = self.remove_between_square_brackets()\n",
    "        txt = self.remove_stopwords()\n",
    "        return txt\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.use_all()\n",
    "    \n",
    "\n",
    "txt = \"\"\" im big fan work many enjoyed movie postal im boll apparently bought rights use far cry long ago even game even\n",
    " people enjoyed killing mercs infiltrating secret research labs located tropical island far something mr boll schemed together\n",
    "   along legion feeling loneley set mr boll invites three countrymen play players go names til udo kier ralf names actually made \n",
    "   selfs pretty big movie tale goes like jack carver played til schweiger carver german hail bratwurst eating however find tils acting movie pretty\n",
    "     people complained really staying true whole carver agenda saw carver first person perspective really know looked like kicking storyline film beyond \n",
    "     see evil mad scientist krieger played udo making gms performing research island reminds vancouver thats right palm trees instead got nice rich even gone \n",
    "     far started cry cannot go wanna stay true bolls shenanigans go see movie disappointed delivers true boll meaning things worth mentioning would imply boll\n",
    "       good work areas film nice boat fighting whole gms squad enters scene everything makes movie far cry reeks scheisse poop wanna take wiff go btw carver gets\n",
    "         annoying sidekick makes wanna shoot first three minutes\"\"\"\n",
    "\n",
    "obj = preprocess_input(text=txt)\n",
    "print(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in tqdm(range(0,len(X)),desc =\"build corpus\"):\n",
    "    stemmed_words = [ps.stem(word) for word in X[i].split()]\n",
    "    text = ' '.join(stemmed_words)\n",
    "    corpus.append(text)\n",
    "    \n",
    "    # print(' '.join(stemmed_words))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 14, 35, 15, 36, 16, 2, 37, 13, 1, 38, 39, 17, 40, 4, 6, 41, 42, 7, 43, 7, 18, 16, 44, 45, 46, 47, 19, 48, 49, 50, 20, 4, 51, 21, 1, 52, 53, 54, 55, 56, 57, 58, 21, 1, 59, 22, 60, 8, 61, 5, 23, 9, 24, 62, 63, 23, 64, 65, 66, 25, 14, 2, 67, 68, 26, 69, 3, 8, 9, 70, 3, 71, 72, 73, 74, 75, 76, 9, 77, 2, 25, 18, 78, 27, 28, 10, 29, 3, 79, 80, 3, 30, 81, 82, 27, 83, 84, 26, 85, 86, 31, 87, 32, 88, 89, 90, 91, 8, 24, 11, 33, 92, 19, 20, 93, 94, 95, 17, 96, 97, 98, 99, 34, 100, 7, 101, 4, 102, 6, 103, 5, 12, 28, 10, 1, 104, 5, 32, 2, 105, 106, 10, 1, 107, 108, 109, 110, 111, 112, 1, 113, 15, 114, 31, 34, 115, 116, 29, 33, 117, 118, 119, 120, 11, 2, 4, 6, 121, 122, 123, 12, 124, 125, 5, 126, 3, 127, 128, 129, 11, 12, 130, 30, 22, 131]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 52,  53,  54,  55,  56,  57,  58,  21,   1,  59,  22,  60,   8,\n",
       "         61,   5,  23,   9,  24,  62,  63,  23,  64,  65,  66,  25,  14,\n",
       "          2,  67,  68,  26,  69,   3,   8,   9,  70,   3,  71,  72,  73,\n",
       "         74,  75,  76,   9,  77,   2,  25,  18,  78,  27,  28,  10,  29,\n",
       "          3,  79,  80,   3,  30,  81,  82,  27,  83,  84,  26,  85,  86,\n",
       "         31,  87,  32,  88,  89,  90,  91,   8,  24,  11,  33,  92,  19,\n",
       "         20,  93,  94,  95,  17,  96,  97,  98,  99,  34, 100,   7, 101,\n",
       "          4, 102,   6, 103,   5,  12,  28,  10,   1, 104,   5,  32,   2,\n",
       "        105, 106,  10,   1, 107, 108, 109, 110, 111, 112,   1, 113,  15,\n",
       "        114,  31,  34, 115, 116,  29,  33, 117, 118, 119, 120,  11,   2,\n",
       "          4,   6, 121, 122, 123,  12, 124, 125,   5, 126,   3, 127, 128,\n",
       "        129,  11,  12, 130,  30,  22, 131]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_vectorize_input(text:str):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words =[ps.stem(word) for word in txt.split()]\n",
    "    tokenizer = Tokenizer()\n",
    "    # print(tokenizer)\n",
    "    tokenizer.fit_on_texts(texts=stemmed_words)\n",
    "    # print(tokenizer)\n",
    "    docs = tokenizer.texts_to_sequences([stemmed_words])\n",
    "    print(docs)\n",
    "    pad_docs = pad_sequences(sequences=docs,maxlen=150,padding='post')\n",
    "    # print(pad_docs)\n",
    "    return np.array(pad_docs)\n",
    "    # return ' '.join(stemmed_words)\n",
    "\n",
    "tokenize_vectorize_input(obj.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' im big fan work many enjoyed movie postal im boll apparently bought rights use far cry long ago even game even\\n people enjoyed killing mercs infiltrating secret research labs located tropical island far something mr boll schemed together\\n   along legion feeling loneley set mr boll invites three countrymen play players go names til udo kier ralf names actually made \\n   selfs pretty big movie tale goes like jack carver played til schweiger carver german hail bratwurst eating however find tils acting movie pretty\\n     people complained really staying true whole carver agenda saw carver first person perspective really know looked like kicking storyline film beyond \\n     see evil mad scientist krieger played udo making gms performing research island reminds vancouver thats right palm trees instead got nice rich even gone \\n     far started cry cannot go wanna stay true bolls shenanigans go see movie disappointed delivers true boll meaning things worth mentioning would imply boll\\n       good work areas film nice boat fighting whole gms squad enters scene everything makes movie far cry reeks scheisse poop wanna take wiff go btw carver gets\\n         annoying sidekick makes wanna shoot first three minut'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 'big',\n",
       " 'fan',\n",
       " 'work',\n",
       " 'mani',\n",
       " 'enjoy',\n",
       " 'movi',\n",
       " 'postal',\n",
       " 'im',\n",
       " 'boll',\n",
       " 'appar',\n",
       " 'bought',\n",
       " 'right',\n",
       " 'use',\n",
       " 'far',\n",
       " 'cri',\n",
       " 'long',\n",
       " 'ago',\n",
       " 'even',\n",
       " 'game',\n",
       " 'even',\n",
       " 'peopl',\n",
       " 'enjoy',\n",
       " 'kill',\n",
       " 'merc',\n",
       " 'infiltr',\n",
       " 'secret',\n",
       " 'research',\n",
       " 'lab',\n",
       " 'locat',\n",
       " 'tropic',\n",
       " 'island',\n",
       " 'far',\n",
       " 'someth',\n",
       " 'mr',\n",
       " 'boll',\n",
       " 'scheme',\n",
       " 'togeth',\n",
       " 'along',\n",
       " 'legion',\n",
       " 'feel',\n",
       " 'loneley',\n",
       " 'set',\n",
       " 'mr',\n",
       " 'boll',\n",
       " 'invit',\n",
       " 'three',\n",
       " 'countrymen',\n",
       " 'play',\n",
       " 'player',\n",
       " 'go',\n",
       " 'name',\n",
       " 'til',\n",
       " 'udo',\n",
       " 'kier',\n",
       " 'ralf',\n",
       " 'name',\n",
       " 'actual',\n",
       " 'made',\n",
       " 'self',\n",
       " 'pretti',\n",
       " 'big',\n",
       " 'movi',\n",
       " 'tale',\n",
       " 'goe',\n",
       " 'like',\n",
       " 'jack',\n",
       " 'carver',\n",
       " 'play',\n",
       " 'til',\n",
       " 'schweiger',\n",
       " 'carver',\n",
       " 'german',\n",
       " 'hail',\n",
       " 'bratwurst',\n",
       " 'eat',\n",
       " 'howev',\n",
       " 'find',\n",
       " 'til',\n",
       " 'act',\n",
       " 'movi',\n",
       " 'pretti',\n",
       " 'peopl',\n",
       " 'complain',\n",
       " 'realli',\n",
       " 'stay',\n",
       " 'true',\n",
       " 'whole',\n",
       " 'carver',\n",
       " 'agenda',\n",
       " 'saw',\n",
       " 'carver',\n",
       " 'first',\n",
       " 'person',\n",
       " 'perspect',\n",
       " 'realli',\n",
       " 'know',\n",
       " 'look',\n",
       " 'like',\n",
       " 'kick',\n",
       " 'storylin',\n",
       " 'film',\n",
       " 'beyond',\n",
       " 'see',\n",
       " 'evil',\n",
       " 'mad',\n",
       " 'scientist',\n",
       " 'krieger',\n",
       " 'play',\n",
       " 'udo',\n",
       " 'make',\n",
       " 'gm',\n",
       " 'perform',\n",
       " 'research',\n",
       " 'island',\n",
       " 'remind',\n",
       " 'vancouv',\n",
       " 'that',\n",
       " 'right',\n",
       " 'palm',\n",
       " 'tree',\n",
       " 'instead',\n",
       " 'got',\n",
       " 'nice',\n",
       " 'rich',\n",
       " 'even',\n",
       " 'gone',\n",
       " 'far',\n",
       " 'start',\n",
       " 'cri',\n",
       " 'cannot',\n",
       " 'go',\n",
       " 'wanna',\n",
       " 'stay',\n",
       " 'true',\n",
       " 'boll',\n",
       " 'shenanigan',\n",
       " 'go',\n",
       " 'see',\n",
       " 'movi',\n",
       " 'disappoint',\n",
       " 'deliv',\n",
       " 'true',\n",
       " 'boll',\n",
       " 'mean',\n",
       " 'thing',\n",
       " 'worth',\n",
       " 'mention',\n",
       " 'would',\n",
       " 'impli',\n",
       " 'boll',\n",
       " 'good',\n",
       " 'work',\n",
       " 'area',\n",
       " 'film',\n",
       " 'nice',\n",
       " 'boat',\n",
       " 'fight',\n",
       " 'whole',\n",
       " 'gm',\n",
       " 'squad',\n",
       " 'enter',\n",
       " 'scene',\n",
       " 'everyth',\n",
       " 'make',\n",
       " 'movi',\n",
       " 'far',\n",
       " 'cri',\n",
       " 'reek',\n",
       " 'scheiss',\n",
       " 'poop',\n",
       " 'wanna',\n",
       " 'take',\n",
       " 'wiff',\n",
       " 'go',\n",
       " 'btw',\n",
       " 'carver',\n",
       " 'get',\n",
       " 'annoy',\n",
       " 'sidekick',\n",
       " 'make',\n",
       " 'wanna',\n",
       " 'shoot',\n",
       " 'first',\n",
       " 'three',\n",
       " 'minut']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words =[ps.stem(word) for word in txt.split()]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13,\n",
      " 14,\n",
      " 35,\n",
      " 15,\n",
      " 36,\n",
      " 16,\n",
      " 2,\n",
      " 37,\n",
      " 13,\n",
      " 1,\n",
      " 38,\n",
      " 39,\n",
      " 17,\n",
      " 40,\n",
      " 4,\n",
      " 6,\n",
      " 41,\n",
      " 42,\n",
      " 7,\n",
      " 43,\n",
      " 7,\n",
      " 18,\n",
      " 16,\n",
      " 44,\n",
      " 45,\n",
      " 46,\n",
      " 47,\n",
      " 19,\n",
      " 48,\n",
      " 49,\n",
      " 50,\n",
      " 20,\n",
      " 4,\n",
      " 51,\n",
      " 21,\n",
      " 1,\n",
      " 52,\n",
      " 53,\n",
      " 54,\n",
      " 55,\n",
      " 56,\n",
      " 57,\n",
      " 58,\n",
      " 21,\n",
      " 1,\n",
      " 59,\n",
      " 22,\n",
      " 60,\n",
      " 8,\n",
      " 61,\n",
      " 5,\n",
      " 23,\n",
      " 9,\n",
      " 24,\n",
      " 62,\n",
      " 63,\n",
      " 23,\n",
      " 64,\n",
      " 65,\n",
      " 66,\n",
      " 25,\n",
      " 14,\n",
      " 2,\n",
      " 67,\n",
      " 68,\n",
      " 26,\n",
      " 69,\n",
      " 3,\n",
      " 8,\n",
      " 9,\n",
      " 70,\n",
      " 3,\n",
      " 71,\n",
      " 72,\n",
      " 73,\n",
      " 74,\n",
      " 75,\n",
      " 76,\n",
      " 9,\n",
      " 77,\n",
      " 2,\n",
      " 25,\n",
      " 18,\n",
      " 78,\n",
      " 27,\n",
      " 28,\n",
      " 10,\n",
      " 29,\n",
      " 3,\n",
      " 79,\n",
      " 80,\n",
      " 3,\n",
      " 30,\n",
      " 81,\n",
      " 82,\n",
      " 27,\n",
      " 83,\n",
      " 84,\n",
      " 26,\n",
      " 85,\n",
      " 86,\n",
      " 31,\n",
      " 87,\n",
      " 32,\n",
      " 88,\n",
      " 89,\n",
      " 90,\n",
      " 91,\n",
      " 8,\n",
      " 24,\n",
      " 11,\n",
      " 33,\n",
      " 92,\n",
      " 19,\n",
      " 20,\n",
      " 93,\n",
      " 94,\n",
      " 95,\n",
      " 17,\n",
      " 96,\n",
      " 97,\n",
      " 98,\n",
      " 99,\n",
      " 34,\n",
      " 100,\n",
      " 7,\n",
      " 101,\n",
      " 4,\n",
      " 102,\n",
      " 6,\n",
      " 103,\n",
      " 5,\n",
      " 12,\n",
      " 28,\n",
      " 10,\n",
      " 1,\n",
      " 104,\n",
      " 5,\n",
      " 32,\n",
      " 2,\n",
      " 105,\n",
      " 106,\n",
      " 10,\n",
      " 1,\n",
      " 107,\n",
      " 108,\n",
      " 109,\n",
      " 110,\n",
      " 111,\n",
      " 112,\n",
      " 1,\n",
      " 113,\n",
      " 15,\n",
      " 114,\n",
      " 31,\n",
      " 34,\n",
      " 115,\n",
      " 116,\n",
      " 29,\n",
      " 33,\n",
      " 117,\n",
      " 118,\n",
      " 119,\n",
      " 120,\n",
      " 11,\n",
      " 2,\n",
      " 4,\n",
      " 6,\n",
      " 121,\n",
      " 122,\n",
      " 123,\n",
      " 12,\n",
      " 124,\n",
      " 125,\n",
      " 5,\n",
      " 126,\n",
      " 3,\n",
      " 127,\n",
      " 128,\n",
      " 129,\n",
      " 11,\n",
      " 12,\n",
      " 130,\n",
      " 30,\n",
      " 22,\n",
      " 131]\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words =[ps.stem(word) for word in txt.split()]\n",
    "# pprint(stemmed_words)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts=stemmed_words)\n",
    "pprint(tokenizer.texts_to_sequences([stemmed_words])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 3s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11051664],\n",
       "       [0.15846196],\n",
       "       [0.05775732],\n",
       "       ...,\n",
       "       [0.05100874],\n",
       "       [0.05775733],\n",
       "       [0.06046253]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15846196], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
